{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e814cdb5-b294-4182-8e66-433d5e735169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data_path = '../data/data_QA.json'\n",
    "with open(data_path, \"r\") as datafile:\n",
    "    data_QA = json.load(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b21e09f-dd9c-4013-8d1a-8b3839c315eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/data_QA_emb.json'\n",
    "with open(data_path, \"r\") as datafile:\n",
    "    data_QA2 = json.load(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0107f9ab-3a29-43e0-aaf5-0643a4876173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['leaderboard_bbh_boolean_expressions', 'leaderboard_bbh_causal_judgement', 'leaderboard_bbh_date_understanding', 'leaderboard_bbh_disambiguation_qa', 'leaderboard_bbh_formal_fallacies', 'leaderboard_bbh_geometric_shapes', 'leaderboard_bbh_hyperbaton', 'leaderboard_bbh_logical_deduction_five_objects', 'leaderboard_bbh_logical_deduction_seven_objects', 'leaderboard_bbh_logical_deduction_three_objects', 'leaderboard_bbh_movie_recommendation', 'leaderboard_bbh_navigate', 'leaderboard_bbh_object_counting', 'leaderboard_bbh_penguins_in_a_table', 'leaderboard_bbh_reasoning_about_colored_objects', 'leaderboard_bbh_ruin_names', 'leaderboard_bbh_salient_translation_error_detection', 'leaderboard_bbh_snarks', 'leaderboard_bbh_sports_understanding', 'leaderboard_bbh_temporal_sequences', 'leaderboard_bbh_tracking_shuffled_objects_five_objects', 'leaderboard_bbh_tracking_shuffled_objects_seven_objects', 'leaderboard_bbh_tracking_shuffled_objects_three_objects', 'leaderboard_bbh_web_of_lies', 'leaderboard_gpqa_diamond', 'leaderboard_gpqa_extended', 'leaderboard_gpqa_main', 'leaderboard_ifeval', 'leaderboard_math_algebra_hard', 'leaderboard_math_counting_and_prob_hard', 'leaderboard_math_geometry_hard', 'leaderboard_math_intermediate_algebra_hard', 'leaderboard_math_num_theory_hard', 'leaderboard_math_prealgebra_hard', 'leaderboard_math_precalculus_hard', 'leaderboard_mmlu_pro', 'leaderboard_musr_murder_mysteries', 'leaderboard_musr_object_placements', 'leaderboard_musr_team_allocation'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_QA2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "133f1163-69fe-4239-b02e-8cfb7261b26a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Evaluate the result of a random Boolean expression.\\\\n\\\\nQ: not ( ( not not True ) ) is\\\\n\\\\nA: False\\n\\nQ: True and False and not True and True is\\\\n\\\\nA: False\\n\\nQ: not not ( not ( False ) ) is\\\\n\\\\nA: True\\n\\nQ: not ( True ) and ( True ) is\\\\n\\\\nA:',\n",
       " 'Evaluate the result of a random Boolean expression.\\\\n\\\\nQ: not ( ( not not True ) ) is\\\\n\\\\nA: False\\n\\nQ: True and False and not True and True is\\\\n\\\\nA: False\\n\\nQ: not not ( not ( False ) ) is\\\\n\\\\nA: True\\n\\nQ: True and not not ( not False ) is\\\\n\\\\nA:',\n",
       " 'Evaluate the result of a random Boolean expression.\\\\n\\\\nQ: not ( ( not not True ) ) is\\\\n\\\\nA: False\\n\\nQ: True and False and not True and True is\\\\n\\\\nA: False\\n\\nQ: not not ( not ( False ) ) is\\\\n\\\\nA: True\\n\\nQ: not True or False or ( False ) is\\\\n\\\\nA:',\n",
       " 'Evaluate the result of a random Boolean expression.\\\\n\\\\nQ: not ( ( not not True ) ) is\\\\n\\\\nA: False\\n\\nQ: True and False and not True and True is\\\\n\\\\nA: False\\n\\nQ: not not ( not ( False ) ) is\\\\n\\\\nA: True\\n\\nQ: False or not ( True ) and False is\\\\n\\\\nA:',\n",
       " 'Evaluate the result of a random Boolean expression.\\\\n\\\\nQ: not ( ( not not True ) ) is\\\\n\\\\nA: False\\n\\nQ: True and False and not True and True is\\\\n\\\\nA: False\\n\\nQ: not not ( not ( False ) ) is\\\\n\\\\nA: True\\n\\nQ: True or not False and True and False is\\\\n\\\\nA:']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_QA['leaderboard_bbh_boolean_expressions']['Qs'][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49ba1711-385e-4e77-a3fc-c12abec00815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['not ( True ) and ( True ) is\\\\n\\\\nA:',\n",
       " 'True and not not ( not False ) is\\\\n\\\\nA:',\n",
       " 'not True or False or ( False ) is\\\\n\\\\nA:',\n",
       " 'False or not ( True ) and False is\\\\n\\\\nA:',\n",
       " 'True or not False and True and False is\\\\n\\\\nA:']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_QA2['leaderboard_bbh_boolean_expressions']['Qs'][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb7f02a-0722-4334-8852-b43dfc3880d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
